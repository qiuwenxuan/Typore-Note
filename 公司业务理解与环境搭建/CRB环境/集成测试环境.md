## 集成测试blk用例CRB环境

### 多设备多队列FIO随即包流通

```
test_nvmedev.py::TestNVMeBlkDev::test_blkpf_nvme_multidev_io_forcekill_vhost --env-type=CRB --env-sub-type=all -vs

pytest test_blkdev.py::TestLegacyBlkDev::test_blkpf_legacy_multidev_io_forcekill_vhost test_blkdev.py::TestModernBlkDev::test_blkpf_modern_multidev_io_forcekill_vhost test_blkdev.py::TestPackedBlkDev::test_blkpf_packed_multidev_io_forcekill_vhost --env-type=CRB --env-sub-type=all -vs
```



```
查看host和N2对应的满足设备类型的设备，选择所有满足条件的设备
	host：ls -l /sys/class/block/vd*
1.host端加载 virtio blk驱动(默认自动加载)，
	手动加载命令：
		modprobe virtio_blk
	查看驱动是否加载成功：
		lsmod | grep virtio_blk
2.创建一个tmux窗口准备fio发流，选择所有满足条件的设备，编辑发流模版文件
	先编辑发流模版文件
		vim fio_test.cfg
	杀掉已存在的发流进程
		ps aux |grep 'fio /root/fio_test.cfg --eta-newline=1 --output=/root/fio_test_output.txt'|grep -v grep | awk '{print $2}' |xargs kill -15
	使用命令查看fio进程
		ps aux |grep 'fio /root/fio_test.cfg --eta-newline=1 --output=/root/fio_test_output.txt'|grep -v grep | awk '{print $2}'
	使用发流命令发流：
		IOENGINE=libaio IODEPTH=64 NUMJOBS=1 BSRANGE=512B-1024k RW=randrw RWMIXREAD=50 RUNTIME=3600 VERIFY=crc32 fio /root/fio_test.cfg --eta-newline=1     --output=/root/fio_test_output.txt
3.lsblk 查看blk设备；
4.准备FIO脚本，配置bs等参数；
5.执行FIO脚本，查看FIO实时读写IOPS打印，以及cat /sys/block/vd*/inflight打印，确认所有blk设备流通；
```

### FIO带流kill -9杀掉vhost进程再拉起，FIO恢复

```
pytest test_blkdev.py::TestLegacyBlkDev::test_blkpf_legacy_multidev_reload_driver test_blkdev.py::TestModernBlkDev::test_blkpf_modern_multidev_io_forcekill_vhost test_blkdev.py::TestPackedBlkDev::test_blkpf_legacy_multidev_reload_driver --env-type=CRB --env-sub-type=all -vs
pytest test_blkdev.py::TestLegacyBlkDev::test_blkpf_legacy_multidev_host_reset test_blkdev.py::TestModernBlkDev::test_blkpf_modern_multidev_io_forcekill_vhost test_blkdev.py::TestPackedBlkDev::test_blkpf_packed_multidev_io_forcekill_vhost --env-type=CRB --env-sub-type=all -vs

1.fio持续发流
2.N2上kill -9 vhost进程
	ps aux |grep 'jmnd_vhost'|grep -v grep| awk '{print $2}' |xargs kill -9
3.查看vhost进程是否杀死
	ps aux |grep 'jmnd_vhost'|grep -v grep| awk '{print $2}' |wc -l
4.启动vhost进程
	cd /usr/share/jmnd/single/debug_script/8net_8blk;./3_start_spdk.sh
5.查看vhost进程是否存在
	ps aux |grep 'jmnd_vhost'|grep -v grep| awk '{print $2}' |wc -l
6.查看fio流是否已恢复
	tmux capture-pane -S -20 -t tmux_fio_test && tmux show-buffer
	

	
查看fio进程命令：
	ps aux |grep 'fio /root/fio_test.cfg --eta-newline=1 --output=/root/fio_test_output.txt'|grep -v grep | awk '{print $2}'
host杀掉fio的进程命令：
	ps aux |grep 'fio /root/fio_test.cfg --eta-newline=1 --output=/root/fio_test_output.txt'|grep -v grep | awk '{print $2}' |xargs kill -15
检查fio发流命令：
	ps aux |grep 'fio /root/fio_test.cfg --eta-newline=1 --output=/root/fio_test_output.txt'|grep -v grep | awk '{print $2}'
```

### host reset，加载virtio-blk驱动，FIO流通

```
pytest test_blkdev.py::TestLegacyBlkDev::test_blkpf_legacy_multidev_host_reset --env-type=CRB --env-sub-type=all -vs
1.满足后端设备类型设备全部fio发流
2.host bmc操作"Powe reset"和"Power Cycle"
	ipmitool -I lanplus -H 10.21.187.92 -U admin -P admin power status # 查看host bmc状态
	ipmitool -I lanplus -H 10.21.187.92 -U admin -P admin power reset # 重启host bmc
	ping 10.21.187.91
3.host重启后，查看状态是否已启动
	N2上执行
	ping 10.21.187.91
	From 10.21.186.168 icmp_seq=80 Destination Host Unreachable
	64 bytes from 10.21.187.91: icmp_seq=81 ttl=64 time=1919 ms

4.先关闭RC超时时间
	setpci -s ae:00.0 68.w=0099
	lspci -s ae:00.0 -vvv |grep TimeoutDis
5.加载virtio_blk驱动
	sysctl -w kernel.hung_task_panic=0   #关闭io hung超时panic
	modprobe virtio_blk
6.加载virtio_net驱动
	modprobe virtio_net
	systemctl stop NetworkManager 
	
```

### 热插拔hotplug用例

热插拔的盘的配置文件路径/usr/share/jmnd/libvirt_xml

```
cd /usr/share/jmnd/libvirt_xml

blk和nvme用例的设备
nvme1n0-vdg 对应blk6.xml文件
/usr/share/jmnd/libvirt_xml/bm_itest_blk/blk6.xml
<disk type='nvme' device='disk' model='virtio'>           
      <driver name='qemu' type='raw' queues='2'/>
      <source type='unix' path='/var/tmp/bm_itest_vhost.8'>
        <reconnect enabled='yes' timeout='10'/>
      </source>
      <target dev='vdg' bus='virtio'/>
    </disk>

热拔命令：qmp detach-device bm disk.xml
热插命令：qmp attach-device bm disk.xml

iconv -f GBK -t UTF-8 blk6.xml -o blk6_utf-8


```







## 集成测试net用例 CRB

### 1、Net设备业务：多设备多队列慢速路径流通(host-host)

[Trex 发流测试记录 - Vinny Yang - Confluence (jaguarmicro.com)](http://space.jaguarmicro.com/pages/viewpage.action?pageId=84992135)

```
host侧
    ls -l /sys/class/net # 查看host端的所有网络设备
    ip addr show # 展示所有的网口

N2侧
    ssh root@127.1 -p 6000
    ovs-vsctl show # 查看dpdk内的网桥   
    
    清流表
    
    ovs-ofctl del-flows br-jmnd                      #清慢表
    ovs-appctl revalidator/purge                    #清快表
ovs-ofctl del-flows br-jmnd;ovs-appctl revalidator/purge
    
    ovs-ofctl add-flow br-jmnd nw_src=185.233.190.2,dl_type=0x0800,in_port=bm_hostnet0,action=output:bm_hostnet5 # 添加流表
    ovs-ofctl dump-flows br-jmnd                    #查看慢表配置
    ovs-appctl dpctl/dump-flows -m                  #查看快表配置
ovs-ofctl dump-flows br-jmnd;ovs-appctl dpctl/dump-flows -m |grep offloaded:yes |grep -v drop # 查看快慢表是否含有生效开启offloaded的流表（是否含有offloaded:yes开启，且过滤掉被丢弃的流表）
	ovs-vsctl del-port prot                        # 删除网桥上的端口

	trex发流文件配置
	cp /home/v2.93/cfg/simple_cfg.yaml /etc/trex_cfg.yaml
	vim /etc/trex_cfg.yaml
	
	发流前准备
   	启动trex_server
   	tmux kill-session -t tmux_trex_server 2>/dev/null;tmux new -s tmux_trex_server -d && tmux send -t tmux_trex_server "cd /home/v2.93/ && ./t-rex-64 -i --no-scapy-server" Enter
   	启动trex_console
   	tmux kill-session -t tmux_trex_console 2>/dev/null;tmux new -s tmux_trex_console -d && tmux send -t tmux_trex_console "cd /home/v2.93/ && ./trex-console -r" Enter
   	
   	启动tcpdump抓包
   	tmux kill-session -t tmux_tcpdump 2>/dev/null;tmux new -s tmux_tcpdump -d && tmux send -t tmux_tcpdump "tcpdump -i ens11 -c 3 -xxx src 185.233.190.2" Enter
   	
   	在trex_console里发包
   	tmux send -t tmux_trex_console "pkt -p 0 -s Ether(dst='04:02:03:04:05:06',src='00:01:00:00:00:02')/IP(src='185.233.190.2',dst='172.0.1.1')" Enter
   	
   	检查tmux_tcpdump内发包的正确性
   	tmux capture-pane -S -10 -t tmux_tcpdump && tmux show-buffer
	查看流表计数
	看慢表
	ovs-ofctl dump-flows br-jmnd
	看快表
	ovs-appctl dpctl/dump-flows -m |grep offloaded:yes |grep -v drop

	针对快表隔5、6s收不到包就刷新，在N2设置OVS表项老化时间可以延长：ovs-vsctl set o . other_config:max-idle=180000 （单位ms）
```

1.首先使用命令查看host的ensx接口

```
ls -l /sys/class/net
```

选取两个host的ens接口用于发流

![image-20231220173708044](http://wenxuanqiu.oss-cn-nanjing.aliyuncs.com/img/20231220173709.png)

1.找到host侧ensX与ovs端口的映射关系：

使用命令找到host端所有网络设备 ensX:pcie_num号 ，并使用命令ip addr show ensX展现所有网卡的mac_addr地址,将信息以字典的形式传入列表

```
ls -l /sys/class/net
ip addr show ensX

字典如下：
[{'dev_name': 'ens10', 'pcie_num': '0000:3e:02.0', 'mac_addr': '52:54:00:00:96:87'}, 
```

![image-20231212234318288](http://wenxuanqiu.oss-cn-nanjing.aliyuncs.com/img/20231212234321.png)

![image-20231213110814360](http://wenxuanqiu.oss-cn-nanjing.aliyuncs.com/img/20231213110815.png)

2.登录N2的6000端口获取N2的设备列表device list，通过pcie号将dev_name匹配到ens网口，将dev_name和guest_features获取到

![image-20231212234607729](http://wenxuanqiu.oss-cn-nanjing.aliyuncs.com/img/20231212234608.png)

通过dev_name与网桥上的端口的iface匹配，得到网桥端口与ensX的映射

```
ovs-vsctl show
```

![image-20231212235115417](http://wenxuanqiu.oss-cn-nanjing.aliyuncs.com/img/20231212235116.png)



	5.查找到ovs_br上的端口的iface值和N2上的dev_name对应的source_path相等，即可找到网桥上的端口和host端的ensX网口的映射关系，当host端选择对应的ensX设备时，对应着N2上的ovs_br dbdk的net端口，我们需要将这两个端口配置发流

随机选择两个ensX设备，修改ymal发流文件，将ensX对应的ovs端口配置好流表，发流端口需要准确配置。

![image-20231213210930991](http://wenxuanqiu.oss-cn-nanjing.aliyuncs.com/img/20231213210932.png)

4.启动trex对ensX发流

	1.启动trex-sever
	
	2.启动trex-console
	
	3.trex-console发流

5.查看快表和慢表，慢表抓到一个包，快表生成了一个抓包记录

### 2、多设备多队列ctrlqueue反复修改active qp 

```

```

测试步骤：

![image-20231215170146385](http://wenxuanqiu.oss-cn-nanjing.aliyuncs.com/img/20231215170147.png)

1.查看host端网络接口

```
ls -l /sys/class/net
```

![image-20231220173708044](http://wenxuanqiu.oss-cn-nanjing.aliyuncs.com/img/20231220173709.png)

2.随机选择一个发流网络接口将他的状态变为up，并配置ip网关

```
ip link set dev ens16 up 

ip addr del 7.7.7.1/24 dev ens9 # 添加网管前需先删除原来的7.7.7.1网关的接口的配置
ip addr add 7.7.7.1/24 dev ens16 # 给端口添加ip网关7.7.7.1
```

3.查看网络接口是否配置成功

```
ip a # 查看所有的网络接口ip
```

![image-20231220173944752](http://wenxuanqiu.oss-cn-nanjing.aliyuncs.com/img/20231220173946.png)

查看单个网络接口ip

```
ip addr show ensX # 查看单个网络接口的配置的ip地址
```

![image-20231220174333215](http://wenxuanqiu.oss-cn-nanjing.aliyuncs.com/img/20231220174334.png)

4.查看host网络接口对应着的N2端的网桥的端口

```
进入N2查看host网络接口对应的网络设备
ovs-vsctl show
```

![image-20231220174515664](http://wenxuanqiu.oss-cn-nanjing.aliyuncs.com/img/20231220174516.png)

![image-20231220175108036](http://wenxuanqiu.oss-cn-nanjing.aliyuncs.com/img/20231220175109.png)

5.找到ensx网口对应的网桥端口后，人为在网桥上配置一个dpdk0端口

N2侧添加端口0000:01:00.1到br-ext，0000:01:00.1为pccu口，该口绑定macport0口

```
ovs-vsctl add-port br-ext dpdk0 -- set Interface dpdk0 type=dpdk options:dpdk-devargs=0000:01:00.1 options:n_txq=8 options:n_rxq=8 options:n_rxq_desc=2048 options:n_txq_desc=2048 ofport_request=1
使用命令查看端口是否配置成功：
ovs-vsctl show
```

![image-20231220175337489](http://wenxuanqiu.oss-cn-nanjing.aliyuncs.com/img/20231220175338.png)

6.N2侧添加流表，macport0到ensX之前流量互通

```
首先清慢快表：
ovs-ofctl del-flows br-ext                          #清慢表
ovs-appctl revalidator/purge                        #清快表

配置流表：
ovs-ofctl add-flow br-ext in_port=dpdk0,action=output:netX
ovs-ofctl add-flow br-ext in_port=netX,action=output:dpdk0


```

7.测试仪接口配置7.7.7.2（该步骤目前在测试仪上手动添加，日后需要去调用接口实现自动化配置）

8.host侧ensX设备通过macport0口往测试仪打流（使用ping的方式，两个ip地址产生包交互）

```
ping 7.7.7.2 -c 3
```

9.host侧查询ensX设备的qp最大值及当前值，与N2侧设置的vnet的qp值对比

```
ethtool -l ensX
```

10.host侧修改ensX设备的active qp num为[1,2,3,4,5,6,7,8]中的任意一个值，假如是4

```
ethtool -L ensX combined 4
```

11.循环执行第8/9/10步100次

